---
title:  "RNN 실습"
---

7-8강 복습
===

순전파 학습
-----------------

```
from torch.utils.data import DataLoader, Dataset
import torch

#간단한 데이터셋 정의
class MyDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
        
#데이터 생성
data = torch.arange(10)  # 간단한 0부터 9까지의 숫자 데이터

DataLoader 사용
dataloader = DataLoader(data, batch_size=2, shuffle=True, num_workers=2)


#데이터 로드와 출력
for batch in dataloader:
    print(batch)
```
```
import torch.nn as nn
import torch

class SimpleNN(nn.Module):
  def __init__(self,n_feature,n_hidden,n_output):
    super(SimpleNN,self).__init__()
    self.hidden1 = nn.Linear(n_feature,n_hidden)
    self.relu = nn.ReLU()
    self.hidden2 = nn.Linear(n_hidden,n_hidden)
    self.output = nn.Linear(n_hidden,n_output)
    self.softmax = nn.Softmax(dim=1)

  def forward(self,x):
    x = self.hidden1(x)
    x = self.relu(x)
    x = self.hidden2(x)
    x = self.relu(x)
    y = self.output(x)
    print(y)
    z = self.softmax(y)
    print(z)
    return z
```
##### nn.Softmax(dim=~)에서 dim은 전에 나온 출력값의 차원이 (1,2,3)일 때 몇번째 인덱스로 계산할 것인지 결정한다.차례대로 인덱스 0,1,2이다.

```
instance = SimpleNN(3,4,2)

x=torch.tensor([[1,2,3],[4,5,6],[7,8,9]],dtype=torch.float32)
instance.forward(x)
```

역전파 학습
----
```
label_data = torch.tensor([[1,0],[1,0],[1,0],[1,0]],dtype=torch.float32)
x=torch.tensor([[1,2],[1.1,2.1],[0.9,1.9],[1.2,2.2]],dtype=torch.float32)
```

```
#모델 지정
model = SimpleNN(2,4,2)

#손실함수 지정
loss_fn = nn.CrossEntropyLoss()

#옵티마이저 저장
optimizer = torch.optim.SGD(model.parameters(),lr=0.01)

num_epochs = 500
for epochs in range(num_epochs):
  z = model.forward(x)
  loss = loss_fn(z,label_data)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  print(f'Epoch:{epochs},loss={loss}')
```

![image](https://github.com/user-attachments/assets/39323a11-206e-495e-b204-b4a5b8888420)

##### 위의 그림에서 x_t 즉, 하나의 타임 스텝에 대하여 (4,5)의 벡터가 들어가게된다.

https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/time-series/Simple_RNN.ipynb에 나온 rnn코드들을 한줄한줄 해석해보자!
---

```
class RNN(nn.Module):
    def __init__(self, input_size, output_size, hidden_dim, n_layers):
        super(RNN, self).__init__()
        
        self.hidden_dim=hidden_dim

        # define an RNN with specified parameters
        # batch_first means that the first dim of the input and output will be the batch_size
        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)
        
        # last, fully-connected layer
        self.fc = nn.Linear(48, output_size)

    def forward(self, x, hidden):
        # x (sequence length, batch_size, input_size)
        # hidden (n_layers, batch_size, hidden_dim)
        # r_out (batch_size, time_step, hidden_size)
        batch_size = x.size(1)
        
        # get RNN outputs
        r_out, hidden = self.rnn(x, hidden)
        # shape output to be (batch_size*seq_length, hidden_dim)
        print(r_out)
        r_out = r_out.reshape(1,48)  
        
        # get final output 
        output = self.fc(r_out)
        
        return output, hidden
```

##### 일단 초기설정과 순전파학습을 위한 코드이다.hidden_dim과 ouput_size를 따로 둔 것으로 보아 은닉상태의 벡터가 곧 출력값이 아니라 선형변환을 한 번 더 거쳐서 최종적으로 출력값이 나오는 듯하다.

## 모델 설계
```
class RNN(nn.Module):
    def __init__(self, input_size, output_size, hidden_dim, n_layers):
        super(RNN, self).__init__()
        
        self.hidden_dim=hidden_dim
        self.n_layers = n_layers

        # define an RNN with specified parameters
        # batch_first means that the first dim of the input and output will be the batch_size
        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)
        
        # last, fully-connected layer
        self.fc = nn.Linear(hidden_dim, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.n_layers, x.size()[1], self.hidden_dim) # 초기 hidden state 설정하기.
        # x (sequence length, batch_size, input_size)
        # hidden (n_layers, batch_size, hidden_dim)
        # r_out (sequence_length,batch_size, hidden_dim)
        batch_size = x.size(1)
        
        # get RNN outputs
        r_out, hidden = self.rnn(x, h0)
        # shape output to be (batch_size*seq_length, hidden_dim)
        print(r_out)
        print(r_out.size())
        r_out = r_out.reshape(-1,hidden_dim)  
        
        # get final output 
        output = self.fc(r_out)
        
        return output, hidden

# decide on hyperparameters
input_size=1 
output_size=1
hidden_dim=32
n_layers=1

# instantiate an RNN
rnn = RNN(input_size, output_size, hidden_dim, n_layers)
print(rnn)

# MSE loss and Adam optimizer with a learning rate of 0.01
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)
```
## 모델 학습

```
# train the RNN
def train(rnn, n_steps, print_every):
    
    
    
    
    for batch_i, step in enumerate(range(n_steps)):
        # defining the training data 
        time_steps = np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1)
        data = np.sin(time_steps)
        data.resize((seq_length + 1, 1)) # input_size=1

        x = data[:-1]
        y = data[1:] #정답값
        
        # convert data into Tensors
        x_tensor = torch.Tensor(x).unsqueeze(1) # unsqueeze gives a 1, batch_size dimension
        x_tensor = x_tensor.reshape(4,5,1)
        y_tensor = torch.Tensor(y)
       

        # outputs from the rnn
        prediction,hidden = rnn(x_tensor)

        ## Representing Memory ##
        # make a new variable for hidden and detach the hidden state from its history
        # this way, we don't backpropagate through the entire history
        #hidden = hidden.detach()

        # calculate the loss
        loss = criterion(prediction, y_tensor)
        # zero gradients
        optimizer.zero_grad()
        
        
        # perform backprop and update weights
        loss.backward()
        #이전 배치에 업데이트된 가중치를 그대로 가지고 간다.(retain_graph=True로 할 경우)
        optimizer.step()

        # display loss and predictions
        if batch_i%print_every == 0:        
            print('Loss: ', loss.item())
            plt.plot(time_steps[1:], x, 'r.') # input
            plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.') # predictions
            plt.show()
    
    return rnn
```

```
# train the rnn and monitor results
n_steps = 75
print_every = 15

#### 자세한 내용은 아래의 링크를 참조하자

https://github.com/Bigime/deep_daiv.toy_project/blob/main/rnn%EC%8B%A4%EC%8A%B5%20(1).ipynb

trained_rnn = train(rnn, n_steps, print_every)
```

